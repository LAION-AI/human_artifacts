{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab25605c-c459-494c-ab85-81cda2d771ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image, ImageOps\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch.autograd import Variable\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6e4d45-01d5-446f-8686-d83c374f504a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80a5179-9a57-4bd1-bd59-972854eeb68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanArtifact_dataloader(Dataset):\n",
    "    \"\"\"\n",
    "    Human artifact data loader.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_folder, is_train=True):\n",
    "        self.is_train = is_train\n",
    "        self._data_folder = data_folder\n",
    "        self.build_dataset()\n",
    "\n",
    "    def build_dataset(self):\n",
    "        if self.is_train:\n",
    "            self._input_folder = os.path.join(self._data_folder, \"train\", 'humans')\n",
    "            self._label_folder = os.path.join(self._data_folder, \"train\", 'masks')\n",
    "            self.train_images = sorted(glob.glob(self._input_folder + \"/*.png\"))\n",
    "            self.train_labels = sorted(glob.glob(self._label_folder + \"/*.png\"))\n",
    "        else:\n",
    "            self._input_folder = os.path.join(self._data_folder, \"test\", 'humans')\n",
    "            self._label_folder = os.path.join(self._data_folder, \"test\", 'masks')\n",
    "            self.test_images = sorted(glob.glob(self._input_folder + \"/*.png\"))\n",
    "            self.test_labels = sorted(glob.glob(self._label_folder + \"/*.png\"))\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.is_train:\n",
    "            return len(self.train_images)\n",
    "        else:\n",
    "            return len(self.test_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if self.is_train:\n",
    "            img_path = self.train_images[idx]\n",
    "            mask_path = self.train_labels[idx]\n",
    "        else:\n",
    "            img_path = self.test_images[idx]\n",
    "            mask_path = self.test_labels[idx]\n",
    "        \n",
    "        # Read image and mask\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        mask = Image.open(mask_path)\n",
    "        mask = ImageOps.fit(mask, (256, 512), Image.BICUBIC)\n",
    "        mask = np.array(mask.convert(\"RGB\"))[:,:,0]\n",
    "        mask = np.where(mask == 255, 1, 0)\n",
    "        mask = np.expand_dims(mask, axis=0)\n",
    "        \n",
    "        transforms_image = transforms.Compose([transforms.Resize((512, 256)), \n",
    "                                               transforms.CenterCrop((512, 256)),\n",
    "                                               transforms.ToTensor(),\n",
    "                                               transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))])\n",
    "        transforms_mask = transforms.Compose([transforms.Resize((512, 256)),\n",
    "                                              transforms.CenterCrop((512, 256)),\n",
    "                                              transforms.ToTensor()])\n",
    "        \n",
    "        # Convert to torch tensors\n",
    "        image = transforms_image(image)\n",
    "        mask = torch.from_numpy(mask)\n",
    "        \n",
    "        sample = {'image': image, \n",
    "                  'mask': mask}\n",
    "        \n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e22591b-a6d6-4861-a20d-8820bb28bfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = HumanArtifact_dataloader(\"../datasets/human_artifacts\")\n",
    "test_dataset = HumanArtifact_dataloader(\"../datasets/human_artifacts\", is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a2b2bf-0505-4a73-9406-80cb18ad246b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=8)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=8)\n",
    "\n",
    "print(\"Training on {} batches/samples\".format(len(train_dataloader)))\n",
    "print(\"Testing on {} batches/samples\".format(len(test_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c471ce-a4b7-43a0-90bf-c65aaca11f40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d9a756-b603-4b9b-945d-950387412ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379ea3e3-69cc-4c2b-9d72-10985d0ab626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8a06aa-c0e2-4152-b0d8-4cf7bad6d156",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = next(iter(train_dataloader))\n",
    "x = dt[\"image\"]\n",
    "y = dt[\"mask\"]\n",
    "\n",
    "print(\"Sample: \", x[0][:,:10][0][0][:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6ec72d-2b5f-485d-a5ba-62459cbaf24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049535d8-65ce-4e4a-9300-1f26f5806523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_img(ten):\n",
    "    ten =(ten[0].permute(1,2,0).detach().cpu().numpy()+1)/2\n",
    "    ten=(ten*255).astype(np.uint8)\n",
    "    return ten\n",
    "\n",
    "\n",
    "a = to_img(x)\n",
    "print(a.shape)\n",
    "plt.imshow(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5c7ec6-0b3b-4dd9-a44e-9ad1649ec3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = to_img(y)\n",
    "plt.imshow(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2184a996-2128-4c47-aa6c-0330e4807d29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
